{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1316415-5f5c-4088-b5ad-8767fff9925d",
   "metadata": {},
   "source": [
    "# Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e07ed78-a252-49a9-a3a1-12a7141b8bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.set_printoptions(edgeitems=2, threshold=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215012bf-f206-4a56-864a-729145cc0c35",
   "metadata": {},
   "source": [
    "Sound can be seen as fluctuations of pressure of a medium, air for instance, at a certain location in time. There are other representations that we'll get into in a minute, but we can think about this as the _raw_, time-domain representation. In order for the human ear to appreciat sound, pressure must fluctuate with a frequency between 20 and 20000 oscillations per sound (measured in Hertz, Hz). More oscillations per second will lead to a higher perceived pitch.\n",
    "\n",
    "By recording pressure fluctuations in time using a microphone and converting every pressure level at each time point into a number (e.g. a 16-bit integer), we can now represent sound as a vector of numbers. This is known as Pluse Code Modulation (PCM), where a continuous signal is both sampled in time and quantized in amplitude. If we want to make sure we hear the highest possible pitch in a recording, we'll have to record our samples at slightly more than twice the maximum audible frequency, i.e. just over 40000 times per second. It is not by chance that audio CD's have a sampling frequency of 44100 hz. This means that a one-hour stereo (i.e. 2 channels) CD track where samples are recorded at 16-bit precision will amount to `2 * 16 * 44100 * 3600 = 5080320000 bit = 605.6 MB` if stored without compression.\n",
    "\n",
    "There are a plethora of audio formats, WAV, AIFF, MP3, AAC being the most popular, where raw audio signals are typically encoded in compressed form by leveraging on both correlation between successive samples in the time series, between the two stereo channels as well as elimination of barely audible frequencies. This can result in dramatic reduction of storage requirements (a one-hour audio file in AAC format takes less than 60 MB). In addition, audio playaers can decode these formats on the fly on dedicated hardware, consuming a tiny amount of power.\n",
    "\n",
    "In our data scientist role we may have to feed audio samples to our network and classify them, or generate captions, for instance. In that case, we won't work with compressed data, rather we'll have to find a way to load an audio file in some format and lay it out as an uncompressed time series in a tensor. Let's do that now.\n",
    "\n",
    "We can download a fair number of environmental sounds at the ESC-50 repository (https://gitbub.com/karoldvl/ESC-50) in the `audio` directory. Let's get `1-100038-A-14.wav` for instance, containing the sound of a bird chirping.\n",
    "\n",
    "In order to load the sound we resort to SciPy, specifically `scipy.io.wavefile.read`, which has the nice property to return data as a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ff79ee8-b11b-46f9-a289-4f9911fd82c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io.wavfile as wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34530694-6fde-4212-be12-5f4409c400fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44100, array([ -388, -3387, -4634, ...,  2289,  1327,    90], dtype=int16))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq, waveform_arr = wavfile.read('../data/p1ch4/audio-chirp/1-100038-A-14.wav')\n",
    "freq, waveform_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a853481-5cbb-4f2a-8bf1-e860d60b99fa",
   "metadata": {},
   "source": [
    "The `read` function returns two outputs, namely the sampling frequency and the waveform as a 16-bit integer 1D array. It's a single 1D array, which tells us that it's a mono recording - we'd have two waveforms (two channels) if the sound were stereo.\n",
    "\n",
    "We can convert the array to a tensor and we're good to go. We might also want to conver the wave form tensor to a float tensor since we're at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9f04692-6c51-42c3-83a6-03c93720edeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([220500])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform = torch.from_numpy(waveform_arr).float()\n",
    "waveform.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e2c4af-95d5-4e4d-900a-27e363e766ae",
   "metadata": {},
   "source": [
    "In a typical dataset, we'll have more than on waveform, and possibly over more than one channel. Depending on the kind of network employed for carrying out a task, for instance a sound classification task, we would be required to lay out the tensor in one of two ways.\n",
    "\n",
    "For architectures based on filtering the 1D signal with cascades of learned filter bankds, such as convolutional networks, we would need to lay out the tensor as `N x C x L`, where `N` is the number of sounds in a dataset, `C` the number of channels and `L` the number of samples in time.\n",
    "\n",
    "Conversely, for architectures that incorporate the notion of temporal sequences, just as recurrent networks we mentioned for text, data needs to be laid out as `L x N x C` - sequence length comes first. Intuitively, this is because the latter architectures take one set of `C` values at a time - the signal is not considered as a whole, but as an individual input chaning in time.\n",
    "\n",
    "Although the most straightforward, this is only one of the ways to represent audio so that it is digestible by a neural network. Another way is turning the audoi signal into a spectrogram.\n",
    "\n",
    "Instead of representing oscillations explicitly in time, we can characterize what at frequencies those oscillations occur for short time intervals. So, for instance, if we pluck the fifth string of our (hopefully tuned) guitar and we focus on 0.1 seconds of that recording, we will see that the waveform oscillates at 440 cycles per second, plus smaller spurious oscillations at different frequencies that make up the timbre of the sound. If we move on to subsequent 0.1 second intervals, we now see that the frequency content doesn't change, but the intensity does, as the sound of our string fades. If we now decide to pluck another string, we will observe new frequencies fading in time.\n",
    "\n",
    "We could indeed build a plot having time in the X-axis, frequencies heard at that time in the Y-axis and encode intersity of those frequencies as a value at that X and Y. Or color. Ok, that starts ot look like an image, right?\n",
    "\n",
    "That's correct, spectograms are a representation of the intensity at each frequency at each point in time. It turns out that one can train convolutional neural networks built for analyzing images (we'll see about those in a couple of chapters) on sound represented as a spectogram.\n",
    "\n",
    "Let's see how we can turn the sound we loaded earlier into a spectrogram. To do that, we need to resort to a method for converting a signal in the time domain into its frequency content. This is known as the Fourier Transform, and the algorithm that allows us to compute it efficiently is the Fast Fourier Transform (FFT), which is one of the most widespread algorithms out there. If we do that consecutively for short bursts of sound in time, we can build out spectrogram column by column.\n",
    "\n",
    "This is the general idea and we won't go into too many details here. Luckily for us SciPy has a function that gets us a shiny spectrogram given an input waveform. We import the `signal` module from SciPy, then provide the `spectrogram` function with the waveform and the sampling frequency that we got previously. The return values are all NumPy arrays, namely frequency `f_arr` (values along the Y axis), time `t_arr` (values along the X axis) and the actual spectrogram `sp_arr` as a 2D array. Turning the latter info a PyTorch tensor is trivial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "308b2b28-1ce7-4cd8-a4ce-2013e2139cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a31a4f-15da-4eda-bf86-aaba2a74f7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([129, 984])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_arr, t_arr, sp_arr = signal.spectrogram(waveform_arr, freq)\n",
    "\n",
    "sp_mono = torch.from_numpy(sp_arr)\n",
    "sp_mono.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d5a2a2-8f10-4c53-ab08-201f8dd57ca2",
   "metadata": {},
   "source": [
    "Dimension are `F x T`, where `F` is frequency and `T` is time.\n",
    "\n",
    "As we mentioned earlier, stereo sound two channels, which will lead to a two-channel spectrogram. Suppose we have two spectrograms, one for each channel. We can convert the two channels separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a557bf2-3768-4a55-bcd7-cb0296d36196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([129, 984]), torch.Size([129, 984]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_left = sp_right = sp_arr\n",
    "sp_left_t = torch.from_numpy(sp_left)\n",
    "sp_right_t = torch.from_numpy(sp_right)\n",
    "sp_left_t.shape, sp_right_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea239717-ef4e-453b-a153-d5f291c641aa",
   "metadata": {},
   "source": [
    "and stack the two tensors along the first dimension to obtain a two channels image of size `C x F x T`, where `C` is number channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09802f55-9bdb-40a7-a8fd-2528470ffc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 129, 984])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_t = torch.stack((sp_left_t, sp_right_t), dim=0)\n",
    "sp_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca56a909-69d5-4bed-af8a-5509b0323052",
   "metadata": {},
   "source": [
    "If we want to build a dataset to use as input for a network, we will stack multiple spectrograms representing multiple sounds in a dataset along the first dimension, leading to a `N x C x F x T` tensor.\n",
    "\n",
    "Such tensor is indistinguishable from what we would build for a dataset set of images, where `F` is represents rows and `T` columns of an image. Indeed, we would tackle a sound classification problem on spectrograms with the exact same networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe500e-cca8-424c-add6-693144ac4aed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
