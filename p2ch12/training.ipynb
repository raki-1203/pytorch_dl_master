{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d58afa2-0b84-4446-8bff-4b697d777a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/heerak/workspace/venv3.8/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from util.util import enumerateWithEstimate\n",
    "from dsets import LunaDataset\n",
    "from util.logconf import logging\n",
    "from model import LunaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b2ecf1b-1561-4b21-a144-f3d0637accb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "log.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cf0db42-24a4-4936-8323-b34cdab7e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for computeBatchLoss and logMetrics to index into metrics_t/metrics_a\n",
    "METRICS_LABEL_NDX = 0\n",
    "METRICS_PRED_NDX = 1\n",
    "METRICS_LOSS_NDX = 2\n",
    "METRICS_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67859a3e-c020-411e-a3aa-200667e794fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunaTrainingApp:\n",
    "    \n",
    "    def __init__(self, sys_argv=None):\n",
    "        if sys_argv is None:\n",
    "            sys_argv = sys.argv[1:]\n",
    "            \n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--num-workers',\n",
    "                            help='Number of worker processes for background data loading',\n",
    "                            default=8,\n",
    "                            type=int,\n",
    "                           )\n",
    "        parser.add_argument('--batch-size',\n",
    "                            help='Batch size to use for training',\n",
    "                            default=32,\n",
    "                            type=int,\n",
    "                           )\n",
    "        parser.add_argument('--epochs',\n",
    "                            help='Number of epochs to train for',\n",
    "                            default=1,\n",
    "                            type=int,\n",
    "                           )\n",
    "        parser.add_argument('--balanced',\n",
    "                            help='Balance the training data to half positive, half negative.',\n",
    "                            action='store_true',\n",
    "                            default=False,\n",
    "                           )\n",
    "        parser.add_argument('--tb-prefix',\n",
    "                            default='p2ch11',\n",
    "                            help='Data prefix to use for Tensorboard run. Defaults to chapter.',\n",
    "                           )\n",
    "        parser.add_argument('comment',\n",
    "                            help='Comment suffix for Tensorboard run.',\n",
    "                            nargs='?',\n",
    "                            default='dwlpt',\n",
    "                           )\n",
    "        \n",
    "        self.cli_args = parser.parse_args(sys_argv)\n",
    "        self.time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "        \n",
    "        self.trn_writer = None\n",
    "        self.val_writer = None\n",
    "        self.totalTrainingSamples_count = 0\n",
    "        \n",
    "        self.augmentation_dict = {}\n",
    "        if self.cli_args.augmented or self.cli_args.augment_flip:\n",
    "            self.augmentation_dict['flip'] = True\n",
    "        if self.cli_args.augmented or self.cli_args.augment_offset:\n",
    "            self.augmentation_dict['offset'] = 0.1  # 경험에서 나온 것이고 더 좋은 값이 존재할 수 있음\n",
    "        if self.cli_args.augmented or self.cli_args.augment_scale:\n",
    "            self.augmentation_dict['scale'] = 0.2  # 경험에서 나온 것이고 더 좋은 값이 존재할 수 있음\n",
    "        if self.cli_args.augmented or self.cli_args.augment_rotate:\n",
    "            self.augmentation_dict['rotate'] = True\n",
    "        if self.cli_args.augmented or self.cli_args.augment_noise:\n",
    "            self.augmentation_dict['noise'] = 25.0  # 경험에서 나온 것이고 더 좋은 값이 존재할 수 있음\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
    "        \n",
    "        self.model = self.initModel()\n",
    "        self.optimizer = self.initOptimzier()\n",
    "        \n",
    "    def initModel(self):\n",
    "        model = LunaModel()\n",
    "        if self.use_cuda:\n",
    "            log.info('Using CUDA; {} devices.'.format(torch.cuda.device_count()))\n",
    "            if torch.cuda.device_count() > 1:  # 복수 개의 GPU를 탐지\n",
    "                model = nn.DataParallel(model)  # 모델을 래핑\n",
    "            model = model.to(self.device)  # GPU에 모델 파라미터 전달\n",
    "        return model\n",
    "    \n",
    "    def initOpitmizer(self):\n",
    "        return SGD(self.model.parameters(), lr=0.001, momentum=0.99)\n",
    "        # return Adam(self.model.parameters())\n",
    "        \n",
    "    def initTrainDl(self):\n",
    "        train_ds = LunaDataset(  # 커스텀 데이터셋\n",
    "            val_stride=10,\n",
    "            isValSet_bool=False,\n",
    "            ratio_int=int(self.cli_args.balanced),  # 파이썬에서 True 가 1로 변환된다.\n",
    "        )\n",
    "        \n",
    "        batch_size = self.cli_args.batch_size\n",
    "        if self.use_cuda:\n",
    "            batch_size *= torch.cuda.device_count()\n",
    "            \n",
    "        train_dl = DataLoader(  # 바로 사용하면 되는 클래스\n",
    "            train_ds,\n",
    "            batch_size=batch_size,  # 알아서 배치로 나뉜다.\n",
    "            num_workers=self.cli_args.num_workers,\n",
    "            pin_memory=self.use_cuda,  # 고정된 메모리 영역이 GPU 쪽으로 빠르게 전송된다.\n",
    "        )\n",
    "        \n",
    "        return train_dl\n",
    "    \n",
    "    def initValDl(self):\n",
    "        val_ds = LunaDataset(\n",
    "            val_stride=10,\n",
    "            isValSet_bool=True,\n",
    "        )\n",
    "        \n",
    "        batch_size = self.cli_args.batch_size\n",
    "        if self.use_cuda:\n",
    "            batch_size *= torch.cuda.device_count()\n",
    "            \n",
    "        val_dl = DataLoader(\n",
    "            val_ds,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=self.cli_args.num_workers,\n",
    "            pin_memory=self.use_cuda,\n",
    "        )\n",
    "        \n",
    "        return val_dl\n",
    "    \n",
    "    def initTensorboardWriters(self):\n",
    "        if self.trn_writer is None:\n",
    "            log_dir = os.path.join('runs', self.cli_args.tb_prefix, self.time_str)\n",
    "            \n",
    "            self.trn_writer = SummaryWriter(\n",
    "                log_dir=log_dir + '-trn_cls-' + self.cli_args.comment)\n",
    "            self.val_writer = SummaryWRiter(\n",
    "                log_dir=log_dir + '-val_cls-' + self.cli_args.comment)\n",
    "            \n",
    "    def main(self):\n",
    "        log.info('Starting {}, {}'.format(type(self).__name__, self.cli_args))\n",
    "        \n",
    "        train_dl = self.initTrainDl()\n",
    "        val_dl = self.initValDl()  # 검증 데이터 로더는 훈련 데이터 로더와 매우 유사하다.\n",
    "        \n",
    "        for epoch_ndx in range(1, self.cli_args.epochs + 1):\n",
    "            trnMetrics_t = self.doTraining(epoch_ndx, train_dl)\n",
    "            self.logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n",
    "            \n",
    "            valMetrics_t = self.doValidation(epoch_ndx, val_dl)\n",
    "            self.logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
    "            \n",
    "    def doTraining(self, epoch_ndx, train_dl):\n",
    "        self.model.train()\n",
    "        trnMetrics_g = torch.zeros(  # 빈 메트릭 배열을 초기화\n",
    "            METRICS_SIZE,\n",
    "            len(train_dl.dataset),\n",
    "            device=self.device,\n",
    "        )\n",
    "        \n",
    "        batch_iter = enumerateWithEstimate(  # 시간을 예측하며 배치 루프를 설정한다.\n",
    "            train_dl,\n",
    "            \"E{} Training\".format(epoch_ndx),\n",
    "            start_ndx=train_dl.num_workers,\n",
    "        )\n",
    "        for batch_ndx, batch_tup in batch_iter:\n",
    "            self.optimizer.zero_grad()  # 남은 가중치 텐서를 해제한다.\n",
    "            \n",
    "            loss_var = self.computeBatchLoss(  # 이 코드는 다음 절에서 구체적으로 살펴본다.\n",
    "                batch_ndx,\n",
    "                batch_tup,\n",
    "                train_dl.batch_size,\n",
    "                trnMetrics_g\n",
    "            )\n",
    "            \n",
    "            # 모델 가중치를 실제로 조정하는 부분\n",
    "            loss_var.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        self.totalTrainingSamples_count += len(train_dl.dataset)\n",
    "        \n",
    "        return trnMetrics_g.to('cpu')\n",
    "    \n",
    "    def doValidation(self, epoch_ndx, val_dl):\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()  # 훈련 때 사용했던 기능은 끈다.\n",
    "            valMetrics_g = torch.zeros(\n",
    "                METRICS_SIZE,\n",
    "                len(val_dl.dataset),\n",
    "                device=self.device,\n",
    "            )\n",
    "            \n",
    "            batch_iter = enumerateWithEstimate(\n",
    "                val_dl,\n",
    "                \"E{} Validation\".format(epoch_ndx),\n",
    "                start_ndx=val_dl.num_workers,\n",
    "            )\n",
    "            for batch_ndx, batch_tup in batch_iter:\n",
    "                self.computeBatchLoss(\n",
    "                    batch_ndx, batch_tup, val_dl.batch_size, valMetrics_g\n",
    "                )\n",
    "                \n",
    "        return valMetrics_g.to('cpu')\n",
    "    \n",
    "    def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g):\n",
    "        input_t, label_t, _series_list, _center_list = batch_tup\n",
    "        \n",
    "        input_g = input_t.to(self.device, non_blocking=True)\n",
    "        label_g = label_t.to(self.device, non_blocking=True)\n",
    "        \n",
    "        logits_g, probability_g = self.model(input_g)\n",
    "        \n",
    "        loss_func = nn.CrossEntropyLoss(reduction='none')  # reduction='none' 으로 샘플별 손실값을 얻는다.\n",
    "        loss_g = loss_func(\n",
    "            logits_g,\n",
    "            label_g[:, 1],  # 원핫 인코딩 클래스의 인덱스\n",
    "        )\n",
    "        \n",
    "        start_ndx = batch_ndx * batch_size\n",
    "        end_ndx = starat_ndx + label_t.size(0)\n",
    "        \n",
    "        # 기울기에 의존적인 메트릭이 없으므로 detach 를 사용한다.\n",
    "        metrics_g[METRICS_LABEL_NDX, start_ndx:end_ndx] = label_g[:, 1].detach()\n",
    "        metrics_g[METRICS_PRED_NDX, start_ndx:end_ndx] = probability_g[:, 1].detach()\n",
    "        metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = loss_g.detach()\n",
    "        \n",
    "        return loss_g.mean()  # 샘플별 손실값을 단일값으로 합친다.\n",
    "    \n",
    "    def logMetrics(\n",
    "        self,\n",
    "        epoch_ndx,\n",
    "        mode_str,\n",
    "        metrics_t,\n",
    "        classificationThreshold=0.5,\n",
    "    ):\n",
    "        self.initTensorboardWriters()\n",
    "        log.info(\"E{} {}\".format(epoch_ndx, type(self).__name__,))\n",
    "        \n",
    "        negLabel_mask = metrics_t[METRICS_LABEL_NDX] <= classificationThreshold\n",
    "        negPred_mask = metrics_t[METRICS_PRED_NDX] <= classificationThreshold\n",
    "        \n",
    "        posLabel_mask = ~negLabel_mask\n",
    "        posPred_mask = ~negPred_mask\n",
    "        \n",
    "        neg_count = int(negLabel_mask.sum())  # 일반 파이썬 정수로 변환\n",
    "        pos_count = int(posLabel_mask.sum())\n",
    "        \n",
    "        trueNeg_count = neg_correct = int((negLabel_mask & negPred_mask).sum())\n",
    "        truePos_count = pos_correct = int((posLabel_mask & posPred_mask).sum())\n",
    "        \n",
    "        falsePos_count = neg_count - neg_correct\n",
    "        flaseNeg_count = pos_count - pos_correct\n",
    "        \n",
    "        metrics_dict = {}\n",
    "        metrics_dict['loss/all'] = metrics_t[METRICS_LOSS_NDX].mean()\n",
    "        metrics_dict['loss/neg'] = metrics_t[METRICS_LOSS_NDX, negLabel_mask].mean()\n",
    "        metrics_dict['loss/pos'] = metrics_t[METRICS_LOSS_NDX, posLabel_mask].mean()\n",
    "        \n",
    "        # np.float32 변환으로 정수 나눗셈을 피한다.\n",
    "        metrics_dict['correct/all'] = (pos_correct + neg_correct) / np.float32(metrics_t.shape[1]) * 100\n",
    "        metrics_dict['correct/neg'] = neg_correct / np.float32(neg_count) * 100\n",
    "        metrics_dict['correct/pos'] = pos_correct / np.float32(pos_count) * 100\n",
    "        \n",
    "        precision = metrics_dict['pr/precision'] = truePos_count / np.float32(truePos_count + falsePos_count)\n",
    "        recall = metrics_dict['pr/recall'] = truePos_count / np.float32(truePos_count + falseNeg_count)\n",
    "        \n",
    "        metrics_dict['pr/f1_score'] = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        log.info(\n",
    "            (\"E{} {:8} {loss/all:.4f} loss, \" \n",
    "             + \"{correct/all:-5.1}% correct, \"\n",
    "             + \"{pr/precision:.4f} precision, \"\n",
    "             + \"{pr/recall:.4f} recall, \"\n",
    "             + \"{pr/f1_score:.4f} f1 score\").format(\n",
    "                epoch_ndx,\n",
    "                mode_str,\n",
    "                **metrics_dict,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        log.info(\n",
    "            (\"E{} {:8} {loss/neg:.4f} loss, \" \n",
    "             + \"{correct/neg:-5.1f}% correct, ({neg_correct:} of {neg_count:})\").format(\n",
    "                epoch_ndx, \n",
    "                mode_str + '_neg', \n",
    "                neg_correct=neg_correct,\n",
    "                neg_count=neg_count,\n",
    "                **metrics_dict,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        log.info(\n",
    "            (\"E{} {:8} {loss/pos:.4f} loss, \" \n",
    "             + \"{correct/pos:-5.1f}% correct ({pos_correct:} of {pos_count:})\").format(\n",
    "                epoch_ndx,\n",
    "                mode_str + '_pos',\n",
    "                pos_correct=pos_correct,\n",
    "                pos_count=pos_count,\n",
    "                **metrics_dict,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        writer = getattr(self, mode_str + '_writer')\n",
    "        \n",
    "        for key, value in metrics_dict.items():\n",
    "            writer.add_scalar(key, value, self.totalTrainingSamples_count)\n",
    "            \n",
    "        writer.add_pr_curve(\n",
    "            'pr',\n",
    "            metrics_t[METRICS_LABEL_NDX],\n",
    "            metrics_t[METRICS_PRED_NDX],\n",
    "            self.totalTrainingSamples_count,\n",
    "        )\n",
    "        \n",
    "        bins = [x / 50.0 for x in range(51)]\n",
    "        \n",
    "        negHist_mask = negLabel_mask & (metrics_t[METRICS_PRED_NDX] > 0.01)\n",
    "        posHist_mask = posLabel_mask & (metrics_t[METRICS_PRED_NDX] < 0.99)\n",
    "        \n",
    "        if negHist_mask.any():\n",
    "            writer.add_histogram(\n",
    "                'is_neg',\n",
    "                metrics_t[METRICS_PRED_NDX, negHist_mask],\n",
    "                self.totalTrainingSamples_count,\n",
    "                bins=bins,\n",
    "            )\n",
    "            \n",
    "        if posHist_mask.any():\n",
    "            writer.add_histogram(\n",
    "                'is_pos',\n",
    "                metrics_t[MEGRICS_PRED_NDX, posHist_mask],\n",
    "                self.totalTrainingSamples_count,\n",
    "                bins=bins,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9057503a-b2f9-434c-96df-2c04958d8fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
